{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Scraping part"
      ],
      "metadata": {
        "id": "_Ah1smzCIwoc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Firstly, we need to install Scrapy library. Let's do it with pip 3.11 version"
      ],
      "metadata": {
        "id": "5Q8E1Y-OMLcT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "255OIKPsIpAu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f0461a3-bd73-4f2a-c72b-b83727eb6de0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scrapy\n",
            "  Downloading Scrapy-2.12.0-py2.py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting Twisted>=21.7.0 (from scrapy)\n",
            "  Downloading twisted-24.11.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: cryptography>=37.0.0 in /usr/local/lib/python3.11/dist-packages (from scrapy) (43.0.3)\n",
            "Collecting cssselect>=0.9.1 (from scrapy)\n",
            "  Downloading cssselect-1.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting itemloaders>=1.0.1 (from scrapy)\n",
            "  Downloading itemloaders-1.3.2-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting parsel>=1.5.0 (from scrapy)\n",
            "  Downloading parsel-1.10.0-py2.py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: pyOpenSSL>=22.0.0 in /usr/local/lib/python3.11/dist-packages (from scrapy) (24.2.1)\n",
            "Collecting queuelib>=1.4.2 (from scrapy)\n",
            "  Downloading queuelib-1.7.0-py2.py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting service-identity>=18.1.0 (from scrapy)\n",
            "  Downloading service_identity-24.2.0-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting w3lib>=1.17.0 (from scrapy)\n",
            "  Downloading w3lib-2.3.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting zope.interface>=5.1.0 (from scrapy)\n",
            "  Downloading zope.interface-7.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting protego>=0.1.15 (from scrapy)\n",
            "  Downloading Protego-0.4.0-py2.py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting itemadapter>=0.1.0 (from scrapy)\n",
            "  Downloading itemadapter-0.11.0-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from scrapy) (24.2)\n",
            "Collecting tldextract (from scrapy)\n",
            "  Downloading tldextract-5.1.3-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: lxml>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from scrapy) (5.3.1)\n",
            "Requirement already satisfied: defusedxml>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from scrapy) (0.7.1)\n",
            "Collecting PyDispatcher>=2.0.5 (from scrapy)\n",
            "  Downloading PyDispatcher-2.0.7-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=37.0.0->scrapy) (1.17.1)\n",
            "Collecting jmespath>=0.9.5 (from itemloaders>=1.0.1->scrapy)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.11/dist-packages (from service-identity>=18.1.0->scrapy) (25.1.0)\n",
            "Requirement already satisfied: pyasn1 in /usr/local/lib/python3.11/dist-packages (from service-identity>=18.1.0->scrapy) (0.6.1)\n",
            "Requirement already satisfied: pyasn1-modules in /usr/local/lib/python3.11/dist-packages (from service-identity>=18.1.0->scrapy) (0.4.1)\n",
            "Collecting automat>=24.8.0 (from Twisted>=21.7.0->scrapy)\n",
            "  Downloading Automat-24.8.1-py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting constantly>=15.1 (from Twisted>=21.7.0->scrapy)\n",
            "  Downloading constantly-23.10.4-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting hyperlink>=17.1.1 (from Twisted>=21.7.0->scrapy)\n",
            "  Downloading hyperlink-21.0.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting incremental>=24.7.0 (from Twisted>=21.7.0->scrapy)\n",
            "  Downloading incremental-24.7.2-py3-none-any.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.11/dist-packages (from Twisted>=21.7.0->scrapy) (4.12.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from zope.interface>=5.1.0->scrapy) (75.1.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from tldextract->scrapy) (3.10)\n",
            "Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from tldextract->scrapy) (2.32.3)\n",
            "Collecting requests-file>=1.4 (from tldextract->scrapy)\n",
            "  Downloading requests_file-2.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.11/dist-packages (from tldextract->scrapy) (3.17.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=37.0.0->scrapy) (2.22)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.1.0->tldextract->scrapy) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.1.0->tldextract->scrapy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.1.0->tldextract->scrapy) (2025.1.31)\n",
            "Downloading Scrapy-2.12.0-py2.py3-none-any.whl (311 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
            "Downloading itemadapter-0.11.0-py3-none-any.whl (11 kB)\n",
            "Downloading itemloaders-1.3.2-py3-none-any.whl (12 kB)\n",
            "Downloading parsel-1.10.0-py2.py3-none-any.whl (17 kB)\n",
            "Downloading Protego-0.4.0-py2.py3-none-any.whl (8.6 kB)\n",
            "Downloading PyDispatcher-2.0.7-py3-none-any.whl (12 kB)\n",
            "Downloading queuelib-1.7.0-py2.py3-none-any.whl (13 kB)\n",
            "Downloading service_identity-24.2.0-py3-none-any.whl (11 kB)\n",
            "Downloading twisted-24.11.0-py3-none-any.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading w3lib-2.3.1-py3-none-any.whl (21 kB)\n",
            "Downloading zope.interface-7.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (259 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m259.8/259.8 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tldextract-5.1.3-py3-none-any.whl (104 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.9/104.9 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Automat-24.8.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading constantly-23.10.4-py3-none-any.whl (13 kB)\n",
            "Downloading hyperlink-21.0.0-py2.py3-none-any.whl (74 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.6/74.6 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading incremental-24.7.2-py3-none-any.whl (20 kB)\n",
            "Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading requests_file-2.1.0-py2.py3-none-any.whl (4.2 kB)\n",
            "Installing collected packages: PyDispatcher, zope.interface, w3lib, queuelib, protego, jmespath, itemadapter, incremental, hyperlink, cssselect, constantly, automat, Twisted, requests-file, parsel, tldextract, service-identity, itemloaders, scrapy\n",
            "Successfully installed PyDispatcher-2.0.7 Twisted-24.11.0 automat-24.8.1 constantly-23.10.4 cssselect-1.2.0 hyperlink-21.0.0 incremental-24.7.2 itemadapter-0.11.0 itemloaders-1.3.2 jmespath-1.0.1 parsel-1.10.0 protego-0.4.0 queuelib-1.7.0 requests-file-2.1.0 scrapy-2.12.0 service-identity-24.2.0 tldextract-5.1.3 w3lib-2.3.1 zope.interface-7.2\n"
          ]
        }
      ],
      "source": [
        "!pip3.11 install scrapy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Creates a directory if it doesn’t already exist and navigates into it.\n",
        "project_name = \"my_scrapy_project\"\n",
        "os.makedirs(project_name, exist_ok=True)\n",
        "os.chdir(project_name)"
      ],
      "metadata": {
        "id": "V2GOw772vZm1"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Secondly, we need to create and start the project\n"
      ],
      "metadata": {
        "id": "d-Z5-KDULu96"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!scrapy startproject wikipedia"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8gMFlXcbBJo",
        "outputId": "7c64d5e0-bf72-4c86-9133-13c3c8094ab1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New Scrapy project 'wikipedia', using template directory '/usr/local/lib/python3.11/dist-packages/scrapy/templates/project', created in:\n",
            "    /content/my_scrapy_project/wikipedia\n",
            "\n",
            "You can start your first spider with:\n",
            "    cd wikipedia\n",
            "    scrapy genspider example example.com\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Then we need to create a spider"
      ],
      "metadata": {
        "id": "FrN7Zph4XsNK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(\"wikipedia\")\n",
        "!scrapy genspider films_spider en.wikipedia.org"
      ],
      "metadata": {
        "id": "DOvevix1nIMl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6948aaf7-19ee-4073-efdc-f33785098173"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created spider 'films_spider' using template 'basic' in module:\n",
            "  wikipedia.spiders.films_spider\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Class of spider with start URL, domain and all scraping functions"
      ],
      "metadata": {
        "id": "Mk-6fpGB7OUr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/my_scrapy_project/wikipedia/wikipedia/spiders/films_spider.py\n",
        "\n",
        "import scrapy\n",
        "import re\n",
        "\n",
        "# Spider class\n",
        "class HighestGrossingFilmsSpider(scrapy.Spider):\n",
        "    name = 'films_spider'\n",
        "    allowed_domains = ['en.wikipedia.org']\n",
        "    start_urls = ['https://en.wikipedia.org/wiki/List_of_highest-grossing_films']\n",
        "\n",
        "    # Extracts the talbe with the films\n",
        "    def parse(self, response):\n",
        "\n",
        "        table = response.xpath('//table[contains(@class, \"wikitable\") and contains(@class, \"sortable\")]')\n",
        "\n",
        "        # Shows error message if table wasn't foundf\n",
        "        if not table:\n",
        "            self.logger.error(\"Table not found...\")\n",
        "            return\n",
        "\n",
        "        # Extracting film data\n",
        "        for row in table.xpath('.//tr')[1:]:\n",
        "            rank = row.xpath('.//td[1]//text()').get()\n",
        "            title = row.xpath('.//th//i//a//text()').get()\n",
        "            worldwide_gross = row.xpath('.//td[3]//text()').getall()\n",
        "            year = row.xpath('.//td[4]//text()').get()\n",
        "            film_url = row.xpath('.//th//i//a/@href').get()\n",
        "\n",
        "            # Cleaning the extracted data\n",
        "            if worldwide_gross:\n",
        "\n",
        "                worldwide_gross = ''.join(worldwide_gross).strip()\n",
        "\n",
        "                dollar_index = worldwide_gross.find('$')\n",
        "\n",
        "                if dollar_index != -1:\n",
        "                    worldwide_gross = worldwide_gross[dollar_index:]\n",
        "                else:\n",
        "                    worldwide_gross = ''\n",
        "\n",
        "            if rank:\n",
        "                rank = rank.strip()\n",
        "            if title:\n",
        "                title = title.strip()\n",
        "            if year:\n",
        "                year = year.strip()\n",
        "\n",
        "            # Opens film pages\n",
        "            if film_url:\n",
        "                film_url = response.urljoin(film_url)\n",
        "                yield scrapy.Request(\n",
        "                    url=film_url,\n",
        "                    callback=self.parse_film_page,\n",
        "                    meta={\n",
        "                        'rank': rank,\n",
        "                        'title': title,\n",
        "                        'worldwide_gross': worldwide_gross,\n",
        "                        'year': year,\n",
        "                    }\n",
        "                )\n",
        "            else:\n",
        "\n",
        "                yield {\n",
        "                    'rank': rank,\n",
        "                    'title': title,\n",
        "                    'worldwide_gross': worldwide_gross,\n",
        "                    'year': year,\n",
        "                    'director': None,\n",
        "                    'country': None,\n",
        "                }\n",
        "\n",
        "    # Extracting film director and country\n",
        "    def parse_film_page(self, response):\n",
        "\n",
        "        director = response.xpath('//th[contains(text(), \"Directed by\")]/following-sibling::td//text()').getall()\n",
        "\n",
        "        # Cleaning director data\n",
        "        director = [\n",
        "            name.strip() for name in director\n",
        "            if name.strip()\n",
        "            and not name.startswith(\".mw-parser-output\")\n",
        "            and not name.startswith(\"[\")\n",
        "            and not name.endswith(\"]\")\n",
        "        ]\n",
        "\n",
        "        director = ''.join(director).strip()\n",
        "\n",
        "        director = re.sub(r'\\d+', '', director)\n",
        "\n",
        "        director = re.sub(r'([a-z])([A-Z])', r'\\1, \\2', director)\n",
        "\n",
        "        director = ', '.join([name.strip() for name in director.split(',') if name.strip()])\n",
        "\n",
        "        country = response.xpath('//th[contains(text(), \"Country\") or contains(text(), \"Countries\")]/following-sibling::td//text()').getall()\n",
        "\n",
        "        if not country:\n",
        "            country = response.xpath('//div[contains(@class, \"infobox\")]//th[contains(text(), \"Country\") or contains(text(), \"Countries\")]/following-sibling::td//text()').getall()\n",
        "\n",
        "        # Cleaning country data\n",
        "        country = [\n",
        "            name.strip() for name in country\n",
        "            if name.strip()\n",
        "            and not name.startswith(\".mw-parser-output\")\n",
        "            and not name.startswith(\"[\")\n",
        "            and not name.endswith(\"]\")\n",
        "        ]\n",
        "\n",
        "        country = ''.join(country).strip()\n",
        "\n",
        "        country = re.sub(r'\\d+', '', country)\n",
        "\n",
        "        country = re.sub(r'([a-z])([A-Z])', r'\\1, \\2', country)\n",
        "\n",
        "        country = ', '.join([name.strip() for name in country.split(',') if name.strip()])\n",
        "\n",
        "        # Collects data for output\n",
        "        yield {\n",
        "            'rank': response.meta['rank'],\n",
        "            'title': response.meta['title'],\n",
        "            'worldwide_gross': response.meta['worldwide_gross'],\n",
        "            'year': response.meta['year'],\n",
        "            'director': director if director else None,\n",
        "            'country': country if country else None,\n",
        "        }"
      ],
      "metadata": {
        "id": "ASeo5amT7JAY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8be367a7-975f-4956-b363-2d2721289129"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/my_scrapy_project/wikipedia/wikipedia/spiders/films_spider.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overwriting data in output.json in case it's not empty\n"
      ],
      "metadata": {
        "id": "Q_xo8zSfagr7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile output.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cxG3WvTKaYuY",
        "outputId": "95a88426-ce93-4077-fe69-81f5adf400c2"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting output.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scraping data from the website to the JSON"
      ],
      "metadata": {
        "id": "2KxwWuUVYTBf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!scrapy crawl films_spider -o output.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zwy_E7-U_Qw2",
        "outputId": "602dc321-9c4d-4f2c-fd2e-0f7840c44fb6"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-03-02 15:13:35 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: wikipedia)\n",
            "2025-03-02 15:13:35 [scrapy.utils.log] INFO: Versions: lxml 5.3.1.0, libxml2 2.12.9, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.11.11 (main, Dec  4 2024, 08:55:07) [GCC 11.4.0], pyOpenSSL 24.2.1 (OpenSSL 3.3.2 3 Sep 2024), cryptography 43.0.3, Platform Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "2025-03-02 15:13:35 [scrapy.addons] INFO: Enabled addons:\n",
            "[]\n",
            "2025-03-02 15:13:35 [asyncio] DEBUG: Using selector: EpollSelector\n",
            "2025-03-02 15:13:35 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\n",
            "2025-03-02 15:13:35 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop\n",
            "2025-03-02 15:13:35 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\n",
            "2025-03-02 15:13:35 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop\n",
            "2025-03-02 15:13:35 [scrapy.extensions.telnet] INFO: Telnet Password: 280c0b3f0e8eb61b\n",
            "2025-03-02 15:13:35 [scrapy.middleware] INFO: Enabled extensions:\n",
            "['scrapy.extensions.corestats.CoreStats',\n",
            " 'scrapy.extensions.telnet.TelnetConsole',\n",
            " 'scrapy.extensions.memusage.MemoryUsage',\n",
            " 'scrapy.extensions.feedexport.FeedExporter',\n",
            " 'scrapy.extensions.logstats.LogStats']\n",
            "2025-03-02 15:13:35 [scrapy.crawler] INFO: Overridden settings:\n",
            "{'BOT_NAME': 'wikipedia',\n",
            " 'FEED_EXPORT_ENCODING': 'utf-8',\n",
            " 'NEWSPIDER_MODULE': 'wikipedia.spiders',\n",
            " 'ROBOTSTXT_OBEY': True,\n",
            " 'SPIDER_MODULES': ['wikipedia.spiders'],\n",
            " 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}\n",
            "2025-03-02 15:13:35 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
            "['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',\n",
            " 'scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
            " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
            " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
            " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
            " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
            " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
            " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
            "2025-03-02 15:13:35 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
            "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
            " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
            " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
            " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
            "2025-03-02 15:13:35 [scrapy.middleware] INFO: Enabled item pipelines:\n",
            "[]\n",
            "2025-03-02 15:13:35 [scrapy.core.engine] INFO: Spider opened\n",
            "2025-03-02 15:13:35 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
            "2025-03-02 15:13:35 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
            "2025-03-02 15:13:36 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://en.wikipedia.org/robots.txt> (referer: None)\n",
            "2025-03-02 15:13:36 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://en.wikipedia.org/wiki/List_of_highest-grossing_films> (referer: None)\n",
            "2025-03-02 15:13:36 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://en.wikipedia.org/wiki/Avatar_(2009_film)> (referer: https://en.wikipedia.org/wiki/List_of_highest-grossing_films)\n",
            "2025-03-02 15:13:36 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://en.wikipedia.org/wiki/Avengers:_Infinity_War> (referer: https://en.wikipedia.org/wiki/List_of_highest-grossing_films)\n",
            "2025-03-02 15:13:37 [scrapy.core.scraper] DEBUG: Scraped from <200 https://en.wikipedia.org/wiki/Avatar_(2009_film)>\n",
            "{'rank': '1', 'title': 'Avatar', 'worldwide_gross': '$2,923,706,026', 'year': '2009', 'director': 'James Cameron', 'country': 'United Kingdom, United States'}\n",
            "2025-03-02 15:13:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://en.wikipedia.org/wiki/Inside_Out_2> (referer: https://en.wikipedia.org/wiki/List_of_highest-grossing_films)\n",
            "2025-03-02 15:13:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://en.wikipedia.org/wiki/Ne_Zha_2> (referer: https://en.wikipedia.org/wiki/List_of_highest-grossing_films)\n",
            "2025-03-02 15:13:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://en.wikipedia.org/wiki/Spider-Man:_No_Way_Home> (referer: https://en.wikipedia.org/wiki/List_of_highest-grossing_films)\n",
            "2025-03-02 15:13:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://en.wikipedia.org/wiki/Jurassic_World> (referer: https://en.wikipedia.org/wiki/List_of_highest-grossing_films)\n",
            "2025-03-02 15:13:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://en.wikipedia.org/wiki/The_Lion_King_(2019_film)> (referer: https://en.wikipedia.org/wiki/List_of_highest-grossing_films)\n",
            "2025-03-02 15:13:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://en.wikipedia.org/wiki/Furious_7> (referer: https://en.wikipedia.org/wiki/List_of_highest-grossing_films)\n",
            "2025-03-02 15:13:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://en.wikipedia.org/wiki/Star_Wars:_The_Force_Awakens> (referer: https://en.wikipedia.org/wiki/List_of_highest-grossing_films)\n",
            "2025-03-02 15:13:37 [scrapy.core.scraper] DEBUG: Scraped from <200 https://en.wikipedia.org/wiki/Avengers:_Infinity_War>\n",
            "{'rank': '6', 'title': 'Avengers: Infinity War', 'worldwide_gross': '$2,048,359,754', 'year': '2018', 'director': 'Anthony Russo, Joe Russo', 'country': 'United States'}\n",
            "2025-03-02 15:13:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://en.wikipedia.org/wiki/The_Avengers_(2012_film)> (referer: https://en.wikipedia.org/wiki/List_of_highest-grossing_films)\n",
            "2025-03-02 15:13:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://en.wikipedia.org/wiki/Titanic_(1997_film)> (referer: https://en.wikipedia.org/wiki/List_of_highest-grossing_films)\n",
            "2025-03-02 15:13:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://en.wikipedia.org/wiki/Avatar:_The_Way_of_Water> (referer: https://en.wikipedia.org/wiki/List_of_highest-grossing_films)\n",
            "2025-03-02 15:13:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://en.wikipedia.org/wiki/Despicable_Me_3> (referer: https://en.wikipedia.org/wiki/List_of_highest-grossing_films)\n",
            "2025-03-02 15:13:37 [scrapy.core.scraper] DEBUG: Scraped from <200 https://en.wikipedia.org/wiki/Inside_Out_2>\n",
            "{'rank': '9', 'title': 'Inside Out 2', 'worldwide_gross': '$1,698,863,816', 'year': '2024', 'director': 'Kelsey Mann', 'country': 'United States'}\n",
            "2025-03-02 15:13:37 [scrapy.core.scraper] DEBUG: Scraped from <200 https://en.wikipedia.org/wiki/Ne_Zha_2>\n",
            "{'rank': '7', 'title': 'Ne Zha 2', 'worldwide_gross': '$1,963,942,560', 'year': '2025', 'director': 'Jiaozi', 'country': 'China'}\n",
            "2025-03-02 15:13:37 [scrapy.core.scraper] DEBUG: Scraped from <200 https://en.wikipedia.org/wiki/Spider-Man:_No_Way_Home>\n",
            "{'rank': '8', 'title': 'Spider-Man: No Way Home', 'worldwide_gross': '$1,922,598,800', 'year': '2021', 'director': 'Jon Watts', 'country': 'United States'}\n",
            "2025-03-02 15:13:37 [scrapy.core.scraper] DEBUG: Scraped from <200 https://en.wikipedia.org/wiki/Jurassic_World>\n",
            "{'rank': '10', 'title': 'Jurassic World', 'worldwide_gross': '$1,671,537,444', 'year': '2015', 'director': 'Colin Trevorrow', 'country': 'United States'}\n",
            "2025-03-02 15:13:37 [scrapy.core.scraper] DEBUG: Scraped from <200 https://en.wikipedia.org/wiki/The_Lion_King_(2019_film)>\n",
            "{'rank': '11', 'title': 'The Lion King', 'worldwide_gross': '$1,656,943,394', 'year': '2019', 'director': 'Jon Favreau', 'country': 'United States'}\n",
            "2025-03-02 15:13:37 [scrapy.core.scraper] DEBUG: Scraped from <200 https://en.wikipedia.org/wiki/Furious_7>\n",
            "{'rank': '13', 'title': 'Furious 7', 'worldwide_gross': '$1,515,341,399', 'year': '2015', 'director': 'James Wan', 'country': 'United States, China'}\n",
            "2025-03-02 15:13:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://en.wikipedia.org/wiki/Incredibles_2> (referer: https://en.wikipedia.org/wiki/List_of_highest-grossing_films)\n",
            "2025-03-02 15:13:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://en.wikipedia.org/wiki/Avengers:_Endgame> (referer: https://en.wikipedia.org/wiki/List_of_highest-grossing_films)\n",
            "2025-03-02 15:13:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://en.wikipedia.org/wiki/Jurassic_Park_(film)> (referer: https://en.wikipedia.org/wiki/List_of_highest-grossing_films)\n",
            "2025-03-02 15:13:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://en.wikipedia.org/wiki/Beauty_and_the_Beast_(2017_film)> (referer: https://en.wikipedia.org/wiki/List_of_highest-grossing_films)\n",
            "2025-03-02 15:13:37 [scrapy.core.scraper] DEBUG: Scraped from <200 https://en.wikipedia.org/wiki/Star_Wars:_The_Force_Awakens>\n",
            "{'rank': '5', 'title': 'Star Wars: The Force Awakens', 'worldwide_gross': '$2,068,223,624', 'year': '2015', 'director': 'J. J. Abrams', 'country': 'United States'}\n",
            "2025-03-02 15:13:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://en.wikipedia.org/wiki/Frozen_(2013_film)> (referer: https://en.wikipedia.org/wiki/List_of_highest-grossing_films)\n",
            "2025-03-02 15:13:37 [scrapy.core.scraper] DEBUG: Scraped from <200 https://en.wikipedia.org/wiki/The_Avengers_(2012_film)>\n",
            "{'rank': '12', 'title': 'The Avengers', 'worldwide_gross': '$1,518,815,515', 'year': '2012', 'director': 'Joss Whedon', 'country': 'United States'}\n",
            "2025-03-02 15:13:37 [scrapy.core.scraper] DEBUG: Scraped from <200 https://en.wikipedia.org/wiki/Titanic_(1997_film)>\n",
            "{'rank': '4', 'title': 'Titanic', 'worldwide_gross': '$2,257,844,554', 'year': '1997', 'director': 'James Cameron', 'country': 'United States'}\n",
            "2025-03-02 15:13:37 [scrapy.core.scraper] DEBUG: Scraped from <200 https://en.wikipedia.org/wiki/Avatar:_The_Way_of_Water>\n",
            "{'rank': '3', 'title': 'Avatar: The Way of Water', 'worldwide_gross': '$2,320,250,281', 'year': '2022', 'director': 'James Cameron', 'country': 'United States'}\n",
            "2025-03-02 15:13:37 [scrapy.core.scraper] DEBUG: Scraped from <200 https://en.wikipedia.org/wiki/Despicable_Me_3>\n",
            "{'rank': '50', 'title': 'Despicable Me 3', 'worldwide_gross': '$1,034,800,131', 'year': '2017', 'director': 'Pierre Coffin, Kyle Balda', 'country': 'United States'}\n",
            "2025-03-02 15:13:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://en.wikipedia.org/wiki/Pirates_of_the_Caribbean:_On_Stranger_Tides> (referer: https://en.wikipedia.org/wiki/List_of_highest-grossing_films)\n",
            "2025-03-02 15:13:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://en.wikipedia.org/wiki/Star_Wars:_Episode_I_%E2%80%93_The_Phantom_Menace> (referer: https://en.wikipedia.org/wiki/List_of_highest-grossing_films)\n",
            "2025-03-02 15:13:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://en.wikipedia.org/wiki/Rogue_One:_A_Star_Wars_Story> (referer: https://en.wikipedia.org/wiki/List_of_highest-grossing_films)\n",
            "2025-03-02 15:13:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://en.wikipedia.org/wiki/Pirates_of_the_Caribbean:_Dead_Man%27s_Chest> (referer: https://en.wikipedia.org/wiki/List_of_highest-grossing_films)\n",
            "2025-03-02 15:13:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://en.wikipedia.org/wiki/Toy_Story_3> (referer: https://en.wikipedia.org/wiki/List_of_highest-grossing_films)\n",
            "2025-03-02 15:13:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://en.wikipedia.org/wiki/Aladdin_(2019_film)> (referer: https://en.wikipedia.org/wiki/List_of_highest-grossing_films)\n",
            "2025-03-02 15:13:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://en.wikipedia.org/wiki/Moana_2> (referer: https://en.wikipedia.org/wiki/List_of_highest-grossing_films)\n",
            "2025-03-02 15:13:38 [scrapy.core.scraper] DEBUG: Scraped from <200 https://en.wikipedia.org/wiki/Incredibles_2>\n",
            "{'rank': '26', 'title': 'Incredibles 2', 'worldwide_gross': '$1,242,805,359', 'year': '2018', 'director': 'Brad Bird', 'country': 'United States'}\n",
            "2025-03-02 15:13:38 [scrapy.core.scraper] DEBUG: Scraped from <200 https://en.wikipedia.org/wiki/Avengers:_Endgame>\n",
            "{'rank': '2', 'title': 'Avengers: Endgame', 'worldwide_gross': '$2,797,501,328', 'year': '2019', 'director': 'Anthony Russo, Joe Russo', 'country': 'United States'}\n",
            "2025-03-02 15:13:38 [scrapy.core.scraper] DEBUG: Scraped from <200 https://en.wikipedia.org/wiki/Jurassic_Park_(film)>\n",
            "{'rank': '49', 'title': 'Jurassic Park', 'worldwide_gross': '$1,037,535,230', 'year': '1993', 'director': 'Steven Spielberg', 'country': 'United States'}\n",
            "2025-03-02 15:13:38 [scrapy.core.scraper] DEBUG: Scraped from <200 https://en.wikipedia.org/wiki/Beauty_and_the_Beast_(2017_film)>\n",
            "{'rank': '25', 'title': 'Beauty and the Beast', 'worldwide_gross': '$1,263,521,126', 'year': '2017', 'director': 'Bill Condon', 'country': 'United States'}\n",
            "2025-03-02 15:13:38 [scrapy.core.scraper] DEBUG: Scraped from <200 https://en.wikipedia.org/wiki/Frozen_(2013_film)>\n",
            "{'rank': '24', 'title': 'Frozen', 'worldwide_gross': '$1,290,000,000', 'year': '2013', 'director': 'Chris Buck, Jennifer Lee', 'country': 'United States'}\n",
            "2025-03-02 15:13:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://en.wikipedia.org/wiki/Toy_Story_4> (referer: https://en.wikipedia.org/wiki/List_of_highest-grossing_films)\n",
            "2025-03-02 15:13:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://en.wikipedia.org/wiki/Star_Wars:_The_Rise_of_Skywalker> (referer: https://en.wikipedia.org/wiki/List_of_highest-grossing_films)\n",
            "2025-03-02 15:13:38 [scrapy.core.scraper] DEBUG: Scraped from <200 https://en.wikipedia.org/wiki/Pirates_of_the_Caribbean:_On_Stranger_Tides>\n",
            "{'rank': '48', 'title': 'Pirates of the Caribbean: On Stranger Tides', 'worldwide_gross': '$1,045,713,802', 'year': '2011', 'director': 'Rob Marshall', 'country': 'United States'}\n",
            "2025-03-02 15:13:38 [scrapy.core.scraper] DEBUG: Scraped from <200 https://en.wikipedia.org/wiki/Star_Wars:_Episode_I_%E2%80%93_The_Phantom_Menace>\n",
            "{'rank': '47', 'title': 'Star Wars: Episode I – The Phantom Menace', 'worldwide_gross': '$1,046,515,409', 'year': '1999', 'director': 'George Lucas', 'country': 'United States'}\n",
            "2025-03-02 15:13:38 [scrapy.core.scraper] DEBUG: Scraped from <200 https://en.wikipedia.org/wiki/Rogue_One:_A_Star_Wars_Story>\n",
            "{'rank': '44', 'title': 'Rogue One: A Star Wars Story', 'worldwide_gross': '$1,057,420,387', 'year': '2016', 'director': 'Gareth Edwards', 'country': 'United States'}\n",
            "2025-03-02 15:13:38 [scrapy.core.scraper] DEBUG: Scraped from <200 https://en.wikipedia.org/wiki/Pirates_of_the_Caribbean:_Dead_Man%27s_Chest>\n",
            "{'rank': '43', 'title': \"Pirates of the Caribbean: Dead Man's Chest\", 'worldwide_gross': '$1,066,179,747', 'year': '2006', 'director': 'Gore Verbinski', 'country': 'United States'}\n",
            "2025-03-02 15:13:38 [scrapy.core.scraper] DEBUG: Scraped from <200 https://en.wikipedia.org/wiki/Toy_Story_3>\n",
            "{'rank': '42', 'title': 'Toy Story 3', 'worldwide_gross': '$1,066,970,811', 'year': '2010', 'director': 'Lee Unkrich', 'country': 'United States'}\n",
            "2025-03-02 15:13:38 [scrapy.core.scraper] DEBUG: Scraped from <200 https://en.wikipedia.org/wiki/Aladdin_(2019_film)>\n",
            "{'rank': '46', 'title': 'Aladdin', 'worldwide_gross': '$1,050,693,953', 'year': '2019', 'director': 'Guy Ritchie', 'country': 'United States'}\n",
            "2025-03-02 15:13:38 [scrapy.core.scraper] DEBUG: Scraped from <200 https://en.wikipedia.org/wiki/Moana_2>\n",
            "{'rank': '45', 'title': 'Moana 2', 'worldwide_gross': '$1,053,578,188', 'year': '2024', 'director': 'David Derrick Jr.Jason Hand, Dana Ledoux Miller', 'country': 'United States'}\n",
            "2025-03-02 15:13:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://en.wikipedia.org/wiki/Transformers:_Dark_of_the_Moon> (referer: https://en.wikipedia.org/wiki/List_of_highest-grossing_films)\n",
            "2025-03-02 15:13:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://en.wikipedia.org/wiki/Transformers:_Age_of_Extinction> (referer: https://en.wikipedia.org/wiki/List_of_highest-grossing_films)\n",
            "2025-03-02 15:13:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://en.wikipedia.org/wiki/Skyfall> (referer: https://en.wikipedia.org/wiki/List_of_highest-grossing_films)\n",
            "2025-03-02 15:13:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://en.wikipedia.org/wiki/Spider-Man:_Far_From_Home> (referer: https://en.wikipedia.org/wiki/List_of_highest-grossing_films)\n",
            "2025-03-02 15:13:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://en.wikipedia.org/wiki/The_Dark_Knight_Rises> (referer: https://en.wikipedia.org/wiki/List_of_highest-grossing_films)\n",
            "2025-03-02 15:13:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://en.wikipedia.org/wiki/Captain_Marvel_(film)> (referer: https://en.wikipedia.org/wiki/List_of_highest-grossing_films)\n",
            "2025-03-02 15:13:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://en.wikipedia.org/wiki/Joker_(2019_film)> (referer: https://en.wikipedia.org/wiki/List_of_highest-grossing_films)\n",
            "2025-03-02 15:13:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://en.wikipedia.org/wiki/The_Lord_of_the_Rings:_The_Return_of_the_King> (referer: https://en.wikipedia.org/wiki/List_of_highest-grossing_films)\n",
            "2025-03-02 15:13:38 [scrapy.core.scraper] DEBUG: Scraped from <200 https://en.wikipedia.org/wiki/Toy_Story_4>\n",
            "{'rank': '41', 'title': 'Toy Story 4', 'worldwide_gross': '$1,073,394,593', 'year': '2019', 'director': 'Josh Cooley', 'country': 'United States'}\n",
            "2025-03-02 15:13:38 [scrapy.core.scraper] DEBUG: Scraped from <200 https://en.wikipedia.org/wiki/Star_Wars:_The_Rise_of_Skywalker>\n",
            "{'rank': '40', 'title': 'Star Wars: The Rise of Skywalker', 'worldwide_gross': '$1,074,144,248', 'year': '2019', 'director': 'J. J. Abrams', 'country': 'United States'}\n",
            "2025-03-02 15:13:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://en.wikipedia.org/wiki/Aquaman_(film)> (referer: https://en.wikipedia.org/wiki/List_of_highest-grossing_films)\n",
            "2025-03-02 15:13:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://en.wikipedia.org/wiki/Minions_(film)> (referer: https://en.wikipedia.org/wiki/List_of_highest-grossing_films)\n",
            "2025-03-02 15:13:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://en.wikipedia.org/wiki/Iron_Man_3> (referer: https://en.wikipedia.org/wiki/List_of_highest-grossing_films)\n",
            "2025-03-02 15:13:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://en.wikipedia.org/wiki/Captain_America:_Civil_War> (referer: https://en.wikipedia.org/wiki/List_of_highest-grossing_films)\n",
            "2025-03-02 15:13:38 [scrapy.core.scraper] DEBUG: Scraped from <200 https://en.wikipedia.org/wiki/Transformers:_Dark_of_the_Moon>\n",
            "{'rank': '35', 'title': 'Transformers: Dark of the Moon', 'worldwide_gross': '$1,123,794,079', 'year': '2011', 'director': 'Michael Bay', 'country': 'United States'}\n",
            "2025-03-02 15:13:38 [scrapy.core.scraper] DEBUG: Scraped from <200 https://en.wikipedia.org/wiki/Transformers:_Age_of_Extinction>\n",
            "{'rank': '37', 'title': 'Transformers: Age of Extinction', 'worldwide_gross': '$1,104,054,072', 'year': '2014', 'director': 'Michael Bay', 'country': 'United States'}\n",
            "2025-03-02 15:13:38 [scrapy.core.scraper] DEBUG: Scraped from <200 https://en.wikipedia.org/wiki/Skyfall>\n",
            "{'rank': '36', 'title': 'Skyfall', 'worldwide_gross': '$1,108,594,137', 'year': '2012', 'director': 'Sam Mendes', 'country': 'United Kingdom, United States'}\n",
            "2025-03-02 15:13:38 [scrapy.core.scraper] DEBUG: Scraped from <200 https://en.wikipedia.org/wiki/Spider-Man:_Far_From_Home>\n",
            "{'rank': '33', 'title': 'Spider-Man: Far From Home', 'worldwide_gross': '$1,132,679,685', 'year': '2019', 'director': 'Jon Watts', 'country': 'United States'}\n",
            "2025-03-02 15:13:38 [scrapy.core.scraper] DEBUG: Scraped from <200 https://en.wikipedia.org/wiki/The_Dark_Knight_Rises>\n",
            "{'rank': '38', 'title': 'The Dark Knight Rises', 'worldwide_gross': '$1,081,169,825', 'year': '2012', 'director': 'Christopher Nolan', 'country': 'United States, United Kingdom'}\n",
            "2025-03-02 15:13:38 [scrapy.core.scraper] DEBUG: Scraped from <200 https://en.wikipedia.org/wiki/Captain_Marvel_(film)>\n",
            "{'rank': '34', 'title': 'Captain Marvel', 'worldwide_gross': '$1,128,274,794', 'year': '2019', 'director': 'Anna Boden, Ryan Fleck', 'country': 'United States'}\n",
            "2025-03-02 15:13:39 [scrapy.core.scraper] DEBUG: Scraped from <200 https://en.wikipedia.org/wiki/Joker_(2019_film)>\n",
            "{'rank': '39', 'title': 'Joker', 'worldwide_gross': '$1,074,458,282', 'year': '2019', 'director': 'Todd Phillips', 'country': 'United States'}\n",
            "2025-03-02 15:13:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://en.wikipedia.org/wiki/The_Fate_of_the_Furious> (referer: https://en.wikipedia.org/wiki/List_of_highest-grossing_films)\n",
            "2025-03-02 15:13:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://en.wikipedia.org/wiki/Jurassic_World:_Fallen_Kingdom> (referer: https://en.wikipedia.org/wiki/List_of_highest-grossing_films)\n",
            "2025-03-02 15:13:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://en.wikipedia.org/wiki/Star_Wars:_The_Last_Jedi> (referer: https://en.wikipedia.org/wiki/List_of_highest-grossing_films)\n",
            "2025-03-02 15:13:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://en.wikipedia.org/wiki/Harry_Potter_and_the_Deathly_Hallows_%E2%80%93_Part_2> (referer: https://en.wikipedia.org/wiki/List_of_highest-grossing_films)\n",
            "2025-03-02 15:13:39 [scrapy.core.scraper] DEBUG: Scraped from <200 https://en.wikipedia.org/wiki/The_Lord_of_the_Rings:_The_Return_of_the_King>\n",
            "{'rank': '32', 'title': 'The Lord of the Rings: The Return of the King', 'worldwide_gross': '$1,147,997,407', 'year': '2003', 'director': 'Peter Jackson', 'country': 'New Zealand, Germany, United States'}\n",
            "2025-03-02 15:13:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://en.wikipedia.org/wiki/Deadpool_%26_Wolverine> (referer: https://en.wikipedia.org/wiki/List_of_highest-grossing_films)\n",
            "2025-03-02 15:13:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://en.wikipedia.org/wiki/Black_Panther_(film)> (referer: https://en.wikipedia.org/wiki/List_of_highest-grossing_films)\n",
            "2025-03-02 15:13:39 [scrapy.core.scraper] DEBUG: Scraped from <200 https://en.wikipedia.org/wiki/Aquaman_(film)>\n",
            "{'rank': '31', 'title': 'Aquaman', 'worldwide_gross': '$1,148,528,393', 'year': '2018', 'director': 'James Wan', 'country': 'United States'}\n",
            "2025-03-02 15:13:39 [scrapy.core.scraper] DEBUG: Scraped from <200 https://en.wikipedia.org/wiki/Minions_(film)>\n",
            "{'rank': '29', 'title': 'Minions', 'worldwide_gross': '$1,159,444,662', 'year': '2015', 'director': 'Pierre Coffin, Kyle Balda', 'country': 'United States'}\n",
            "2025-03-02 15:13:39 [scrapy.core.scraper] DEBUG: Scraped from <200 https://en.wikipedia.org/wiki/Iron_Man_3>\n",
            "{'rank': '28', 'title': 'Iron Man 3', 'worldwide_gross': '$1,214,811,252', 'year': '2013', 'director': 'Shane Black', 'country': 'United States'}\n",
            "2025-03-02 15:13:39 [scrapy.core.scraper] DEBUG: Scraped from <200 https://en.wikipedia.org/wiki/Captain_America:_Civil_War>\n",
            "{'rank': '30', 'title': 'Captain America: Civil War', 'worldwide_gross': '$1,153,337,496', 'year': '2016', 'director': 'Anthony Russo, Joe Russo', 'country': 'United States'}\n",
            "2025-03-02 15:13:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://en.wikipedia.org/wiki/The_Super_Mario_Bros._Movie> (referer: https://en.wikipedia.org/wiki/List_of_highest-grossing_films)\n",
            "2025-03-02 15:13:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://en.wikipedia.org/wiki/Avengers:_Age_of_Ultron> (referer: https://en.wikipedia.org/wiki/List_of_highest-grossing_films)\n",
            "2025-03-02 15:13:39 [scrapy.core.scraper] DEBUG: Scraped from <200 https://en.wikipedia.org/wiki/The_Fate_of_the_Furious>\n",
            "{'rank': '27', 'title': 'The Fate of the Furious', 'worldwide_gross': '$1,238,764,765', 'year': '2017', 'director': 'F. Gary Gray', 'country': 'United States, China'}\n",
            "2025-03-02 15:13:39 [scrapy.core.scraper] DEBUG: Scraped from <200 https://en.wikipedia.org/wiki/Jurassic_World:_Fallen_Kingdom>\n",
            "{'rank': '23', 'title': 'Jurassic World: Fallen Kingdom', 'worldwide_gross': '$1,308,473,425', 'year': '2018', 'director': 'J. A. Bayona', 'country': 'China, United States'}\n",
            "2025-03-02 15:13:39 [scrapy.core.scraper] DEBUG: Scraped from <200 https://en.wikipedia.org/wiki/Star_Wars:_The_Last_Jedi>\n",
            "{'rank': '22', 'title': 'Star Wars: The Last Jedi', 'worldwide_gross': '$1,332,539,889', 'year': '2017', 'director': 'Rian Johnson', 'country': 'United States'}\n",
            "2025-03-02 15:13:39 [scrapy.core.scraper] DEBUG: Scraped from <200 https://en.wikipedia.org/wiki/Harry_Potter_and_the_Deathly_Hallows_%E2%80%93_Part_2>\n",
            "{'rank': '20', 'title': 'Harry Potter and the Deathly Hallows – Part 2', 'worldwide_gross': '$1,342,139,727', 'year': '2011', 'director': 'David Yates', 'country': 'United Kingdom, United States'}\n",
            "2025-03-02 15:13:39 [scrapy.core.scraper] DEBUG: Scraped from <200 https://en.wikipedia.org/wiki/Deadpool_%26_Wolverine>\n",
            "{'rank': '21', 'title': 'Deadpool & Wolverine', 'worldwide_gross': '$1,338,073,645', 'year': '2024', 'director': 'Shawn Levy', 'country': 'United States'}\n",
            "2025-03-02 15:13:39 [scrapy.core.scraper] DEBUG: Scraped from <200 https://en.wikipedia.org/wiki/Black_Panther_(film)>\n",
            "{'rank': '19', 'title': 'Black Panther', 'worldwide_gross': '$1,347,280,838', 'year': '2018', 'director': 'Ryan Coogler', 'country': 'United States'}\n",
            "2025-03-02 15:13:39 [scrapy.core.scraper] DEBUG: Scraped from <200 https://en.wikipedia.org/wiki/The_Super_Mario_Bros._Movie>\n",
            "{'rank': '18', 'title': 'The Super Mario Bros. Movie', 'worldwide_gross': '$1,362,566,989', 'year': '2023', 'director': 'Aaron Horvath, Michael Jelenic', 'country': 'United States'}\n",
            "2025-03-02 15:13:39 [scrapy.core.scraper] DEBUG: Scraped from <200 https://en.wikipedia.org/wiki/Avengers:_Age_of_Ultron>\n",
            "{'rank': '17', 'title': 'Avengers: Age of Ultron', 'worldwide_gross': '$1,402,809,540', 'year': '2015', 'director': 'Joss Whedon', 'country': 'United States'}\n",
            "2025-03-02 15:13:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://en.wikipedia.org/wiki/Frozen_2> (referer: https://en.wikipedia.org/wiki/List_of_highest-grossing_films)\n",
            "2025-03-02 15:13:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://en.wikipedia.org/wiki/Top_Gun:_Maverick> (referer: https://en.wikipedia.org/wiki/List_of_highest-grossing_films)\n",
            "2025-03-02 15:13:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://en.wikipedia.org/wiki/Barbie_(film)> (referer: https://en.wikipedia.org/wiki/List_of_highest-grossing_films)\n",
            "2025-03-02 15:13:39 [scrapy.core.scraper] DEBUG: Scraped from <200 https://en.wikipedia.org/wiki/Frozen_2>\n",
            "{'rank': '15', 'title': 'Frozen 2', 'worldwide_gross': '$1,450,026,933', 'year': '2019', 'director': 'Chris Buck, Jennifer Lee', 'country': 'United States'}\n",
            "2025-03-02 15:13:39 [scrapy.core.scraper] DEBUG: Scraped from <200 https://en.wikipedia.org/wiki/Top_Gun:_Maverick>\n",
            "{'rank': '14', 'title': 'Top Gun: Maverick', 'worldwide_gross': '$1,495,696,292', 'year': '2022', 'director': 'Joseph Kosinski', 'country': 'United States'}\n",
            "2025-03-02 15:13:40 [scrapy.core.scraper] DEBUG: Scraped from <200 https://en.wikipedia.org/wiki/Barbie_(film)>\n",
            "{'rank': '16', 'title': 'Barbie', 'worldwide_gross': '$1,447,038,421', 'year': '2023', 'director': 'Greta Gerwig', 'country': 'United States, United Kingdom'}\n",
            "2025-03-02 15:13:40 [scrapy.core.engine] INFO: Closing spider (finished)\n",
            "2025-03-02 15:13:40 [scrapy.extensions.feedexport] INFO: Stored json feed (50 items) in: output.json\n",
            "2025-03-02 15:13:40 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
            "{'downloader/request_bytes': 24292,\n",
            " 'downloader/request_count': 52,\n",
            " 'downloader/request_method_count/GET': 52,\n",
            " 'downloader/response_bytes': 6097989,\n",
            " 'downloader/response_count': 52,\n",
            " 'downloader/response_status_count/200': 52,\n",
            " 'elapsed_time_seconds': 4.041532,\n",
            " 'feedexport/success_count/FileFeedStorage': 1,\n",
            " 'finish_reason': 'finished',\n",
            " 'finish_time': datetime.datetime(2025, 3, 2, 15, 13, 40, 37424, tzinfo=datetime.timezone.utc),\n",
            " 'httpcompression/response_bytes': 35908081,\n",
            " 'httpcompression/response_count': 52,\n",
            " 'item_scraped_count': 50,\n",
            " 'items_per_minute': None,\n",
            " 'log_count/DEBUG': 107,\n",
            " 'log_count/INFO': 11,\n",
            " 'memusage/max': 267206656,\n",
            " 'memusage/startup': 267206656,\n",
            " 'request_depth_max': 1,\n",
            " 'response_received_count': 52,\n",
            " 'responses_per_minute': None,\n",
            " 'robotstxt/request_count': 1,\n",
            " 'robotstxt/response_count': 1,\n",
            " 'robotstxt/response_status_count/200': 1,\n",
            " 'scheduler/dequeued': 51,\n",
            " 'scheduler/dequeued/memory': 51,\n",
            " 'scheduler/enqueued': 51,\n",
            " 'scheduler/enqueued/memory': 51,\n",
            " 'start_time': datetime.datetime(2025, 3, 2, 15, 13, 35, 995892, tzinfo=datetime.timezone.utc)}\n",
            "2025-03-02 15:13:40 [scrapy.core.engine] INFO: Spider closed (finished)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DataBase part"
      ],
      "metadata": {
        "id": "9oRB1KxWYaTx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating database using SQLite and fullfiling it with the scraped data from the JSON"
      ],
      "metadata": {
        "id": "ya1-zV07Ydvj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile films.db"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "prbvG1YLbGho",
        "outputId": "f4e69648-743f-4a78-c8fd-341eb54e5513"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting films.db\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "import json\n",
        "\n",
        "# Connect to (or create) the SQLite database file\n",
        "conn = sqlite3.connect('films.db')\n",
        "\n",
        "# Initialize cursor object to work with database\n",
        "cursor = conn.cursor()\n",
        "\n",
        "# Creates films database\n",
        "cursor.execute('''\n",
        "CREATE TABLE IF NOT EXISTS films (\n",
        "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "    title TEXT NOT NULL,\n",
        "    release_year INTEGER,\n",
        "    director TEXT,\n",
        "    box_office TEXT,\n",
        "    country TEXT\n",
        ")\n",
        "''')\n",
        "\n",
        "# Opens json file with scraped data\n",
        "with open('output.json', 'r', encoding='utf-8') as file:\n",
        "    films_data = json.load(file)\n",
        "\n",
        "# Inserts films data into the database\n",
        "for film in films_data:\n",
        "\n",
        "    box_office = film['worldwide_gross'].replace('$', '')\n",
        "\n",
        "    cursor.execute('''\n",
        "    INSERT INTO films (title, release_year, director, box_office, country)\n",
        "    VALUES (?, ?, ?, ?, ?)\n",
        "    ''', (film['title'], int(film['year']), film['director'], box_office, film['country']))\n",
        "\n",
        "# Saves data to the database\n",
        "conn.commit()\n",
        "\n",
        "# Closes the connection to the database\n",
        "conn.close()"
      ],
      "metadata": {
        "id": "BkPaOeRSHAfH"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extracting all the data from DataBase to JSON file"
      ],
      "metadata": {
        "id": "DDLkp1YyZFA6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Reconnect to the database file\n",
        "conn = sqlite3.connect('films.db')\n",
        "\n",
        "# Initialize cursor object to work with database\n",
        "cursor = conn.cursor()\n",
        "\n",
        "# Retrieves all data from the database\n",
        "cursor.execute('SELECT * FROM films')\n",
        "rows = cursor.fetchall()\n",
        "\n",
        "# Converting a data from the databse to the JSON\n",
        "films_list = []\n",
        "for row in rows:\n",
        "    film = {\n",
        "        \"id\": row[0],\n",
        "        \"title\": row[1],\n",
        "        \"release_year\": row[2],\n",
        "        \"director\": row[3],\n",
        "        \"box_office\": row[4],\n",
        "        \"country\": row[5]\n",
        "    }\n",
        "    films_list.append(film)\n",
        "\n",
        "# Closing the database connection\n",
        "conn.close()\n",
        "\n",
        "# Writing data to the JSON file\n",
        "with open('films_exported.json', 'w', encoding='utf-8') as json_file:\n",
        "    json.dump(films_list, json_file, ensure_ascii=False, indent=4)"
      ],
      "metadata": {
        "id": "7zVL8N8JHrEe"
      },
      "execution_count": 30,
      "outputs": []
    }
  ]
}